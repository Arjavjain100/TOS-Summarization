{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7IJm32nxcO5e9d6K7Cfrc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arjavjain100/TOS-Summarization/blob/wandb/Wandb_init.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Q_jYwPW43l"
      },
      "outputs": [],
      "source": [
        "!pip install wandb onnx -Uq\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install rouge\n",
        "!git clone https://github.com/Arjavjain100/TOS-Summarization.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer, Trainer, TrainingArguments, pipeline, PretrainedConfig\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.deterministic = True\n",
        "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
        "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
        "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
        "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset location\n",
        "filename = \"/content/TOS-Summarization/Dataset/all_v1_transpose.csv\""
      ],
      "metadata": {
        "id": "osh26l_jW_bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb"
      ],
      "metadata": {
        "id": "97j2lJa_XBT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "id": "2RqIj20HXCxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change this every run\n",
        "config = dict(\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    learning_rate=0.005,\n",
        "    model_name ='nsi319/legal-pegasus')"
      ],
      "metadata": {
        "id": "RpQTVEXYXD7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"pytorch-demo\", config=hyperparameters):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, train_loader, test_loader, criterion, optimizer = make(config)\n",
        "      print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model, train_loader, criterion, optimizer, config)\n",
        "\n",
        "      # and test its final performance\n",
        "      test(model, test_loader)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "EVKkFvJ4XKmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make(config):\n",
        "    # Make the data\n",
        "\n",
        "    train_texts, train_labels, test_texts, test_labels = get_data()\n",
        "\n",
        "    train_loader, test_loader = make_loader(config.model_name, config.batch_size, train_texts, train_labels, test_texts, test_labels)\n",
        "\n",
        "    # Make the model\n",
        "    model = PegasusForConditionalGeneration.from_pretrained(config.model_name).to(device)\n",
        "\n",
        "\n",
        "    # Make the loss and optimizer (Arnav Please tell)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, train_loader, test_loader, criterion, optimizer"
      ],
      "metadata": {
        "id": "X5ptNTssXLVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "    df = pd.read_csv(filename)\n",
        "    df = df[['original_text','reference_summary']]\n",
        "    df.rename(columns = {'original_text':'source', 'reference_summary':'target'}, inplace = True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "    train_texts, train_labels = list(X_train), list(y_train)\n",
        "    test_texts, test_labels = list(X_test), list(y_test)\n",
        "\n",
        "    return train_texts, train_labels, test_texts, test_labels\n",
        "\n",
        "\n",
        "def make_loader(model_name, batch_size, train_texts, train_labels, test_texts, test_labels):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    prepare_test = False if test_texts is None or test_labels is None else True\n",
        "\n",
        "    def tokenize_data(texts, labels):\n",
        "      encodings = tokenizer(texts, truncation=True, padding=True, max_length = 600)\n",
        "      decodings = tokenizer(labels, truncation=True, padding=True, max_length = 256)\n",
        "      dataset_tokenized = PegasusDataset(encodings, decodings)\n",
        "      return dataset_tokenized\n",
        "\n",
        "    train_dataset = tokenize_data(train_texts, train_labels)\n",
        "    test_dataset = tokenize_data(test_texts, test_labels) if prepare_test else None\n",
        "\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              shuffle=True)\n",
        "\n",
        "    test_loader = torch.util.data.DataLoader(dataset=test_dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "LoxFmqtJXUY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pegasus Transformer Dataset\n",
        "\n",
        "class PegasusDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels['input_ids'][idx])  # torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels['input_ids'])\n"
      ],
      "metadata": {
        "id": "o3JR5GciXVvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, criterion, optimizer, config):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(loader) * config.epochs\n",
        "    example_ct = 0  # number of examples seen\n",
        "    batch_ct = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        for _, (images, labels) in enumerate(loader):\n",
        "\n",
        "            loss = train_batch(images, labels, model, optimizer, criterion)\n",
        "            example_ct +=  len(images)\n",
        "            batch_ct += 1\n",
        "\n",
        "            # Report metrics every 25th batch\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "\n",
        "def train_batch(images, labels, model, optimizer, criterion):\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass ➡\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "3LcqrmCpXYOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    # Where the magic happens\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
      ],
      "metadata": {
        "id": "UAJwmuQMXhDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print(f\"Accuracy of the model on the {total} \" +\n",
        "              f\"test images: {correct / total:%}\")\n",
        "\n",
        "        wandb.log({\"test_accuracy\": correct / total})\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    torch.onnx.export(model, images, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")"
      ],
      "metadata": {
        "id": "LaPOTk_hXl16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_pipeline(config)"
      ],
      "metadata": {
        "id": "LCC3cfw2Xqab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}